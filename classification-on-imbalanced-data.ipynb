{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JfLUlawto_D"
   },
   "source": [
    "# Classification on imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mthoSGBAOoX-"
   },
   "source": [
    "Part of the code was taken from the [tensorflow tutorial](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data).\n",
    "We use [Keras](../../guide/keras/overview.ipynb) to define the model and [class weights](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model) to help the model learn from the imbalanced data.\n",
    "\n",
    "This demonstrates how to classify a highly imbalanced dataset in which the number of examples in one class greatly outnumbers the examples in another. The aim is to detect a mere 10001 attack packets from 298,278 packets in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRHmSyHxEIhN"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JM7hDSNClfoK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/08/03 16:27:49 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of keras. If you encounter errors during autologging, try upgrading / downgrading keras to a supported version, or try upgrading MLflow.\n"
     ]
    }
   ],
   "source": [
    "#%%writefile anomaly-detector.py\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix, plot_precision_recall_curve\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "import time\n",
    "mlflow.keras.autolog()\n",
    "\n",
    "#np.random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "c8o1FHzD-_y_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  16\n"
     ]
    }
   ],
   "source": [
    "#%%writefile -a anomaly-detector.py\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile -a anomaly-detector.py\n",
    "\n",
    "def print_table(title, scores_orig, scores_under, scores_over):\n",
    "    metrics = {\n",
    "        'F1': 'test_f1',\n",
    "        'ROC-AUC': 'test_roc_auc',\n",
    "        'Average Precision': 'test_average_precision',\n",
    "        'Balanced Accuracy': 'test_balanced_accuracy',\n",
    "        'Precision': 'test_precision',\n",
    "        'Recall': 'test_recall',\n",
    "        'Fit Time': 'fit_time',\n",
    "    }\n",
    "\n",
    "    print(\"== %s: == \" % (title))\n",
    "    print()\n",
    "    print(\"Metric,Original Mean,Original Std,Undersampled Mean,Undersampled Std,Oversampled Mean,Oversampled Std\")\n",
    "    \n",
    "    for key, value in metrics.items():\n",
    "        print(\"%s,%.5f,%.5f,%.5f,%.5f,%.5f,%.5f\" % (key, scores_orig[value].mean(), scores_orig[value].std(), scores_under[value].mean(), scores_under[value].std(), scores_over[value].mean(), scores_over[value].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3iZVjziKHmX"
   },
   "source": [
    "## Data processing and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frameLen</th>\n",
       "      <th>vlan50</th>\n",
       "      <th>vlan60</th>\n",
       "      <th>vlan70</th>\n",
       "      <th>vlan80</th>\n",
       "      <th>isnanVlan</th>\n",
       "      <th>ipFlag</th>\n",
       "      <th>isnanIP</th>\n",
       "      <th>ipTtl</th>\n",
       "      <th>portFeatureSrc2404</th>\n",
       "      <th>...</th>\n",
       "      <th>isnanTPC</th>\n",
       "      <th>xTcpTdrLen</th>\n",
       "      <th>tcpWinSize</th>\n",
       "      <th>tcpPduSize</th>\n",
       "      <th>isnanPduSize</th>\n",
       "      <th>asduTypeid013</th>\n",
       "      <th>asduTypeid036</th>\n",
       "      <th>x104NaN</th>\n",
       "      <th>asduCause01</th>\n",
       "      <th>asduCause03</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attack</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>288277</td>\n",
       "      <td>288277</td>\n",
       "      <td>288277</td>\n",
       "      <td>288277</td>\n",
       "      <td>288277</td>\n",
       "      <td>288277</td>\n",
       "      <td>288277</td>\n",
       "      <td>288277</td>\n",
       "      <td>288277</td>\n",
       "      <td>288277</td>\n",
       "      <td>...</td>\n",
       "      <td>288277</td>\n",
       "      <td>288277</td>\n",
       "      <td>288277</td>\n",
       "      <td>288277</td>\n",
       "      <td>288277</td>\n",
       "      <td>288277</td>\n",
       "      <td>288277</td>\n",
       "      <td>288277</td>\n",
       "      <td>288277</td>\n",
       "      <td>288277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>...</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>...</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>...</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2501</td>\n",
       "      <td>2501</td>\n",
       "      <td>2501</td>\n",
       "      <td>2501</td>\n",
       "      <td>2501</td>\n",
       "      <td>2501</td>\n",
       "      <td>2501</td>\n",
       "      <td>2501</td>\n",
       "      <td>2501</td>\n",
       "      <td>2501</td>\n",
       "      <td>...</td>\n",
       "      <td>2501</td>\n",
       "      <td>2501</td>\n",
       "      <td>2501</td>\n",
       "      <td>2501</td>\n",
       "      <td>2501</td>\n",
       "      <td>2501</td>\n",
       "      <td>2501</td>\n",
       "      <td>2501</td>\n",
       "      <td>2501</td>\n",
       "      <td>2501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        frameLen  vlan50  vlan60  vlan70  vlan80  isnanVlan  ipFlag  isnanIP  \\\n",
       "attack                                                                         \n",
       "0         288277  288277  288277  288277  288277     288277  288277   288277   \n",
       "1           2500    2500    2500    2500    2500       2500    2500     2500   \n",
       "2           2500    2500    2500    2500    2500       2500    2500     2500   \n",
       "3           2500    2500    2500    2500    2500       2500    2500     2500   \n",
       "4           2501    2501    2501    2501    2501       2501    2501     2501   \n",
       "\n",
       "         ipTtl  portFeatureSrc2404  ...  isnanTPC  xTcpTdrLen  tcpWinSize  \\\n",
       "attack                              ...                                     \n",
       "0       288277              288277  ...    288277      288277      288277   \n",
       "1         2500                2500  ...      2500        2500        2500   \n",
       "2         2500                2500  ...      2500        2500        2500   \n",
       "3         2500                2500  ...      2500        2500        2500   \n",
       "4         2501                2501  ...      2501        2501        2501   \n",
       "\n",
       "        tcpPduSize  isnanPduSize  asduTypeid013  asduTypeid036  x104NaN  \\\n",
       "attack                                                                    \n",
       "0           288277        288277         288277         288277   288277   \n",
       "1             2500          2500           2500           2500     2500   \n",
       "2             2500          2500           2500           2500     2500   \n",
       "3             2500          2500           2500           2500     2500   \n",
       "4             2501          2501           2501           2501     2501   \n",
       "\n",
       "        asduCause01  asduCause03  \n",
       "attack                            \n",
       "0            288277       288277  \n",
       "1              2500         2500  \n",
       "2              2500         2500  \n",
       "3              2500         2500  \n",
       "4              2501         2501  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%writefile -a anomaly-detector.py\n",
    "\n",
    "# Import training data\n",
    "dataset_train = pd.read_csv(\"../data/raw/energy-informatics-2020/csvDataFeaturesTrain.csv\", delimiter=';')\n",
    "dataset_test = pd.read_csv(\"../data/raw/energy-informatics-2020/csvDataFeaturesTest.csv\", delimiter=';')\n",
    "\n",
    "#data_train = dataset_test.append(dataset_test[dataset_test['attack']==1])\n",
    "X = dataset_train.append(dataset_test)\n",
    "X.groupby('attack').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Dataset\n",
    "- attack = 0 means normal\n",
    "- attack = 1 means anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[288277  10001]\n"
     ]
    }
   ],
   "source": [
    "#%%writefile -a anomaly-detector.py\n",
    "\n",
    "X['attack'] = X['attack'].map({0:0, 1:1, 2:1, 3:1, 4:1})\n",
    "print(np.bincount(X['attack']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWKB_CVZFLpB"
   },
   "source": [
    "### Examine the class label imbalance\n",
    "\n",
    "Let's look at the dataset imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HCJFrtuY2iLF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "    Total: 298278\n",
      "    Positive: 10001 (3.35% of total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%writefile -a anomaly-detector.py\n",
    "\n",
    "neg, pos = np.bincount(X['attack'])\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnLKFQDsCBUg"
   },
   "source": [
    "This shows the small fraction of positive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(298278, 21)\n",
      "(20002, 21)\n",
      "[10001 10001]\n",
      "(576554, 21)\n",
      "[288277 288277]\n"
     ]
    }
   ],
   "source": [
    "#%%writefile -a anomaly-detector.py\n",
    "\n",
    "# Use a utility from sklearn to split and shuffle our dataset.\n",
    "y = X.pop('attack')\n",
    "X\n",
    "\n",
    "X_under, y_under = RandomUnderSampler(sampling_strategy='majority').fit_resample(X, y)\n",
    "X_over, y_over = RandomOverSampler().fit_resample(X, y)\n",
    "\n",
    "print(X.shape)\n",
    "print(X_under.shape)\n",
    "print(np.bincount(y_under))\n",
    "print(X_over.shape)\n",
    "print(np.bincount(y_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset\n",
      "  > Class=0 : 288277/298278 (96.6%)\n",
      "  > Class=1 : 10001/298278 (3.4%)\n",
      "Undersampled Dataset\n",
      "  > Class=0 : 10001/20002 (50.0%)\n",
      "  > Class=1 : 10001/20002 (50.0%)\n",
      "Oversampled Dataset\n",
      "  > Class=0 : 288277/576554 (50.0%)\n",
      "  > Class=1 : 288277/576554 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "#%%writefile -a anomaly-detector.py\n",
    "\n",
    "def print_balance(y, title):\n",
    "    classes = np.unique(y)\n",
    "    total = len(y)\n",
    "    print(title)\n",
    "    for c in classes:\n",
    "        n_examples = len(y[y==c])\n",
    "        percent = n_examples / total * 100\n",
    "        print('  > Class=%d : %d/%d (%.1f%%)' % (c, n_examples, total, percent))\n",
    "\n",
    "print_balance(y, \"Original Dataset\")\n",
    "print_balance(y_under, \"Undersampled Dataset\")\n",
    "print_balance(y_over, \"Oversampled Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model and metrics\n",
    "\n",
    "Define a function that creates a simple neural network with a densly connected hidden layer, a [dropout](https://developers.google.com/machine-learning/glossary/#dropout_regularization) layer to reduce overfitting, and an output sigmoid layer that returns the probability of a transaction being fraudulent: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3JQDzUqT3UYG"
   },
   "outputs": [],
   "source": [
    "#%%writefile -a anomaly-detector.py\n",
    "\n",
    "# Create a TensorBoard callback\n",
    "logs = \"./logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
    "                                                 histogram_freq = 1,\n",
    "                                                 profile_batch = '500,520')\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve,      \n",
    "]\n",
    "\n",
    "def make_model(metrics=METRICS):   \n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(64, activation='relu', input_shape=(21,)),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(16, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss=keras.losses.BinaryCrossentropy(), metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                1408      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 6,737\n",
      "Trainable params: 6,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(make_model().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SU0GX6E6mieP"
   },
   "source": [
    "### Understanding useful metrics\n",
    "\n",
    "Notice that there are a few metrics defined above that can be computed by the model that will be helpful when evaluating the performance.\n",
    "\n",
    "\n",
    "\n",
    "*   **False** negatives and **false** positives are samples that were **incorrectly** classified\n",
    "*   **True** negatives and **true** positives are samples that were **correctly** classified\n",
    "*   **Accuracy** is the percentage of examples correctly classified\n",
    ">   $\\frac{\\text{true samples}}{\\text{total samples}}$\n",
    "*   **Precision** is the percentage of **predicted** positives that were correctly classified\n",
    ">   $\\frac{\\text{true positives}}{\\text{true positives + false positives}}$\n",
    "*   **Recall** is the percentage of **actual** positives that were correctly classified\n",
    ">   $\\frac{\\text{true positives}}{\\text{true positives + false negatives}}$\n",
    "*   **AUC** refers to the Area Under the Curve of a Receiver Operating Characteristic curve (ROC-AUC). This metric is equal to the probability that a classifier will rank a random positive sample higher than a random negative sample.\n",
    "\n",
    "Note: Accuracy is not a helpful metric for this task. You can 99.8%+ accuracy on this task by predicting False all the time.  \n",
    "\n",
    "Read more:\n",
    "*  [True vs. False and Positive vs. Negative](https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative)\n",
    "*  [Accuracy](https://developers.google.com/machine-learning/crash-course/classification/accuracy)\n",
    "*   [Precision and Recall](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall)\n",
    "*   [ROC-AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYdhSAoaF_TK"
   },
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDbltVPg2m2q"
   },
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ouUkwPcGQsy3"
   },
   "outputs": [],
   "source": [
    "#%%writefile -a anomaly-detector.py\n",
    "\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsA_7SEntRaV"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile -a anomaly-detector.py\n",
    "\n",
    "# Source: https://scikit-learn.org/0.21/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile -a anomaly-detector.py\n",
    "\n",
    "def cross_validate_model(pipeline, X, y, cv, title):\n",
    "    print_balance(y, title)\n",
    "    scores = cross_validate(pipeline, X.values, y.values, cv=kfold, scoring=('f1', 'roc_auc', 'average_precision', 'balanced_accuracy', 'precision', 'recall'), return_estimator=True, return_train_score=True)\n",
    "    print(\"    > F1: %.3f%% (+/-%.3f%%)\" % (scores['test_f1'].mean()*100, scores['test_f1'].std()*100))        \n",
    "    print(\"    > ROC-AUC: %.3f%% (+/-%.3f%%)\" % (scores['test_roc_auc'].mean()*100, scores['test_roc_auc'].std()*100))\n",
    "    print(\"    > Average Precision: %.3f%% (+/-%.3f%%)\" % (scores['test_average_precision'].mean()*100, scores['test_average_precision'].std()*100))\n",
    "    print(\"    > Balanced Accuracy: %.3f%% (+/-%.3f%%)\" % (scores['test_balanced_accuracy'].mean()*100, scores['test_balanced_accuracy'].std()*100))\n",
    "    print(\"    > Precision: %.3f%% (+/-%.3f%%)\" % (scores['test_precision'].mean()*100, scores['test_precision'].std()*100))\n",
    "    print(\"    > Recall: %.3f%% (+/-%.3f%%)\" % (scores['test_recall'].mean()*100, scores['test_recall'].std()*100))\n",
    "    print(\"    > Fit Time: %.3f s (+/-%.3f s)\" % (scores['fit_time'].mean(), scores['fit_time'].std()))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile -a anomaly-detector.py\n",
    "\n",
    "# Create stratified KFold Splits\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset\n",
      "  > Class=0 : 288277/298278 (96.6%)\n",
      "  > Class=1 : 10001/298278 (3.4%)\n",
      "    > F1: 99.880% (+/-0.068%)\n",
      "    > ROC-AUC: 99.994% (+/-0.016%)\n",
      "    > Average Precision: 99.953% (+/-0.045%)\n",
      "    > Balanced Accuracy: 99.880% (+/-0.068%)\n",
      "    > Precision: 100.000% (+/-0.000%)\n",
      "    > Recall: 99.760% (+/-0.136%)\n",
      "    > Fit Time: 6.200 s (+/-0.156 s)\n",
      "Undersampled Dataset\n",
      "  > Class=0 : 10001/20002 (50.0%)\n",
      "  > Class=1 : 10001/20002 (50.0%)\n",
      "    > F1: 99.880% (+/-0.068%)\n",
      "    > ROC-AUC: 99.994% (+/-0.015%)\n",
      "    > Average Precision: 99.993% (+/-0.015%)\n",
      "    > Balanced Accuracy: 99.880% (+/-0.068%)\n",
      "    > Precision: 100.000% (+/-0.000%)\n",
      "    > Recall: 99.760% (+/-0.136%)\n",
      "    > Fit Time: 0.412 s (+/-0.006 s)\n",
      "Oversampled Dataset\n",
      "  > Class=0 : 288277/576554 (50.0%)\n",
      "  > Class=1 : 288277/576554 (50.0%)\n",
      "    > F1: 99.881% (+/-0.013%)\n",
      "    > ROC-AUC: 99.999% (+/-0.000%)\n",
      "    > Average Precision: 99.998% (+/-0.000%)\n",
      "    > Balanced Accuracy: 99.882% (+/-0.013%)\n",
      "    > Precision: 100.000% (+/-0.000%)\n",
      "    > Recall: 99.763% (+/-0.027%)\n",
      "    > Fit Time: 13.457 s (+/-0.107 s)\n",
      "== Random Forest Classifier: == \n",
      "\n",
      "Metric,Original,Undersampled,Oversampled,Fit Time\n",
      "F1,0.99880,0.99880,0.99881\n",
      "ROC-AUC,0.99994,0.99994,0.99999\n",
      "Average Precision,0.99953,0.99993,0.99998\n",
      "Balanced Accuracy,0.99880,0.99880,0.99882\n",
      "Precision,1.00000,1.00000,1.00000\n",
      "Recall,0.99760,0.99760,0.99763\n",
      "Fit Time,6.19979,0.41161,13.45703\n"
     ]
    }
   ],
   "source": [
    "#%%writefile -a anomaly-detector.py\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_original = cross_validate_model(make_pipeline(StandardScaler(), RandomForestClassifier()), X, y, kfold, \"Original Dataset\")\n",
    "rf_under = cross_validate_model(make_pipeline(StandardScaler(), RandomForestClassifier()), X_under, y_under, kfold, \"Undersampled Dataset\")\n",
    "rf_over = cross_validate_model(make_pipeline(StandardScaler(), RandomForestClassifier()), X_over, y_over, kfold, \"Oversampled Dataset\")\n",
    "\n",
    "print_table(\"Random Forest Classifier\", rf_original, rf_under, rf_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Random Forest Classifier: == \n",
      "\n",
      "Metric,Original Mean,Original Std,Undersampled Mean,Undersampled Std,Oversampled Mean,Oversampled Std\n",
      "F1,0.99880,0.00068,0.99880,0.00068,0.99881,0.00013\n",
      "ROC-AUC,0.99994,0.00016,0.99994,0.00015,0.99999,0.00000\n",
      "Average Precision,0.99953,0.00045,0.99993,0.00015,0.99998,0.00000\n",
      "Balanced Accuracy,0.99880,0.00068,0.99880,0.00068,0.99882,0.00013\n",
      "Precision,1.00000,0.00000,1.00000,0.00000,1.00000,0.00000\n",
      "Recall,0.99760,0.00136,0.99760,0.00136,0.99763,0.00027\n",
      "Fit Time,6.19979,0.15602,0.41161,0.00589,13.45703,0.10737\n"
     ]
    }
   ],
   "source": [
    "print_table(\"Random Forest Classifier\", rf_original, rf_under, rf_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset\n",
      "  > Class=0 : 288277/298278 (96.6%)\n",
      "  > Class=1 : 10001/298278 (3.4%)\n",
      "    > F1: 99.880% (+/-0.075%)\n",
      "    > ROC-AUC: 99.994% (+/-0.015%)\n",
      "    > Average Precision: 99.953% (+/-0.040%)\n",
      "    > Balanced Accuracy: 99.880% (+/-0.075%)\n",
      "    > Precision: 100.000% (+/-0.000%)\n",
      "    > Recall: 99.760% (+/-0.150%)\n",
      "    > Fit Time: 0.397 s (+/-0.020 s)\n",
      "Undersampled Dataset\n",
      "  > Class=0 : 10001/20002 (50.0%)\n",
      "  > Class=1 : 10001/20002 (50.0%)\n",
      "    > F1: 99.880% (+/-0.072%)\n",
      "    > ROC-AUC: 99.994% (+/-0.015%)\n",
      "    > Average Precision: 99.993% (+/-0.015%)\n",
      "    > Balanced Accuracy: 99.880% (+/-0.071%)\n",
      "    > Precision: 100.000% (+/-0.000%)\n",
      "    > Recall: 99.760% (+/-0.143%)\n",
      "    > Fit Time: 0.016 s (+/-0.001 s)\n",
      "Oversampled Dataset\n",
      "  > Class=0 : 288277/576554 (50.0%)\n",
      "  > Class=1 : 288277/576554 (50.0%)\n",
      "    > F1: 99.881% (+/-0.011%)\n",
      "    > ROC-AUC: 99.999% (+/-0.000%)\n",
      "    > Average Precision: 99.998% (+/-0.000%)\n",
      "    > Balanced Accuracy: 99.882% (+/-0.011%)\n",
      "    > Precision: 100.000% (+/-0.000%)\n",
      "    > Recall: 99.763% (+/-0.022%)\n",
      "    > Fit Time: 0.775 s (+/-0.009 s)\n",
      "== Decision Tree: == \n",
      "\n",
      "Metric,Original,Undersampled,Oversampled,Fit Time\n",
      "F1,0.99880,0.99880,0.99881\n",
      "ROC-AUC,0.99994,0.99994,0.99999\n",
      "Average Precision,0.99953,0.99993,0.99998\n",
      "Balanced Accuracy,0.99880,0.99880,0.99882\n",
      "Precision,1.00000,1.00000,1.00000\n",
      "Recall,0.99760,0.99760,0.99763\n",
      "Fit Time,0.39661,0.01617,0.77534\n"
     ]
    }
   ],
   "source": [
    "#%%writefile -a anomaly-detector.py\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "dt_orig = cross_validate_model(make_pipeline(StandardScaler(), tree.DecisionTreeClassifier()), X, y, kfold, \"Original Dataset\")\n",
    "dt_under = cross_validate_model(make_pipeline(StandardScaler(), tree.DecisionTreeClassifier()), X_under, y_under, kfold, \"Undersampled Dataset\")\n",
    "dt_over = cross_validate_model(make_pipeline(StandardScaler(), tree.DecisionTreeClassifier()), X_over, y_over, kfold, \"Oversampled Dataset\")\n",
    "\n",
    "print_table(\"Decision Tree\", dt_orig, dt_under, dt_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Decision Tree: == \n",
      "\n",
      "Metric,Original Mean,Original Std,Undersampled Mean,Undersampled Std,Oversampled Mean,Oversampled Std\n",
      "F1,0.99880,0.00075,0.99880,0.00072,0.99881,0.00011\n",
      "ROC-AUC,0.99994,0.00015,0.99994,0.00015,0.99999,0.00000\n",
      "Average Precision,0.99953,0.00040,0.99993,0.00015,0.99998,0.00000\n",
      "Balanced Accuracy,0.99880,0.00075,0.99880,0.00071,0.99882,0.00011\n",
      "Precision,1.00000,0.00000,1.00000,0.00000,1.00000,0.00000\n",
      "Recall,0.99760,0.00150,0.99760,0.00143,0.99763,0.00022\n",
      "Fit Time,0.39661,0.02043,0.01617,0.00081,0.77534,0.00924\n"
     ]
    }
   ],
   "source": [
    "print_table(\"Decision Tree\", dt_orig, dt_under, dt_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Model (5 Hidden Layers, 1 Output Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 500, Batch Size: 4096\n",
      "Original Dataset\n",
      "  > Class=0 : 288277/298278 (96.6%)\n",
      "  > Class=1 : 10001/298278 (3.4%)\n",
      "    > F1: 91.076% (+/-8.564%)\n",
      "    > ROC-AUC: 99.991% (+/-0.016%)\n",
      "    > Average Precision: 99.862% (+/-0.105%)\n",
      "    > Balanced Accuracy: 99.452% (+/-0.228%)\n",
      "    > Precision: 85.114% (+/-14.870%)\n",
      "    > Recall: 99.640% (+/-0.441%)\n",
      "    > Fit Time: 275.737 s (+/-2.700 s)\n",
      "Undersampled Dataset\n",
      "  > Class=0 : 10001/20002 (50.0%)\n",
      "  > Class=1 : 10001/20002 (50.0%)\n",
      "    > F1: 99.699% (+/-0.095%)\n",
      "    > ROC-AUC: 99.989% (+/-0.021%)\n",
      "    > Average Precision: 99.991% (+/-0.012%)\n",
      "    > Balanced Accuracy: 99.700% (+/-0.095%)\n",
      "    > Precision: 100.000% (+/-0.000%)\n",
      "    > Recall: 99.400% (+/-0.190%)\n",
      "    > Fit Time: 26.596 s (+/-0.308 s)\n",
      "Oversampled Dataset\n",
      "  > Class=0 : 288277/576554 (50.0%)\n",
      "  > Class=1 : 288277/576554 (50.0%)\n",
      "    > F1: 99.715% (+/-0.019%)\n",
      "    > ROC-AUC: 99.996% (+/-0.001%)\n",
      "    > Average Precision: 99.993% (+/-0.002%)\n",
      "    > Balanced Accuracy: 99.716% (+/-0.019%)\n",
      "    > Precision: 99.999% (+/-0.002%)\n",
      "    > Recall: 99.433% (+/-0.039%)\n",
      "    > Fit Time: 525.867 s (+/-3.543 s)\n",
      "== DNN: == \n",
      "\n",
      "Metric,Original,Undersampled,Oversampled,Fit Time\n",
      "F1,0.91076,0.99699,0.99715\n",
      "ROC-AUC,0.99991,0.99989,0.99996\n",
      "Average Precision,0.99862,0.99991,0.99993\n",
      "Balanced Accuracy,0.99452,0.99700,0.99716\n",
      "Precision,0.85114,1.00000,0.99999\n",
      "Recall,0.99640,0.99400,0.99433\n",
      "Fit Time,275.73685,26.59632,525.86692\n"
     ]
    }
   ],
   "source": [
    "#%%writefile -a anomaly-detector.py\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "print(\"Epochs: %s, Batch Size: %s\" % (EPOCHS, BATCH_SIZE))\n",
    "dnn_original = cross_validate_model(make_pipeline(StandardScaler(), KerasClassifier(build_fn=make_model, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0)), X, y, kfold, \"Original Dataset\")\n",
    "dnn_under = cross_validate_model(make_pipeline(StandardScaler(), KerasClassifier(build_fn=make_model, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0)), X_under, y_under, kfold, \"Undersampled Dataset\")\n",
    "dnn_over = cross_validate_model(make_pipeline(StandardScaler(), KerasClassifier(build_fn=make_model, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0)), X_over, y_over, kfold, \"Oversampled Dataset\")\n",
    "\n",
    "print_table(\"DNN\", dnn_original, dnn_under, dnn_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== DNN: == \n",
      "\n",
      "Metric,Original Mean,Original Std,Undersampled Mean,Undersampled Std,Oversampled Mean,Oversampled Std\n",
      "F1,0.91076,0.08564,0.99699,0.00095,0.99715,0.00019\n",
      "ROC-AUC,0.99991,0.00016,0.99989,0.00021,0.99996,0.00001\n",
      "Average Precision,0.99862,0.00105,0.99991,0.00012,0.99993,0.00002\n",
      "Balanced Accuracy,0.99452,0.00228,0.99700,0.00095,0.99716,0.00019\n",
      "Precision,0.85114,0.14870,1.00000,0.00000,0.99999,0.00002\n",
      "Recall,0.99640,0.00441,0.99400,0.00190,0.99433,0.00039\n",
      "Fit Time,275.73685,2.70026,26.59632,0.30760,525.86692,3.54290\n"
     ]
    }
   ],
   "source": [
    "print_table(\"DNN\", dnn_original, dnn_under, dnn_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Model with adjusted weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for class 0 (normal): 0.52\n",
      "Weight for class 1 (attack): 14.91\n",
      "Original Dataset\n",
      "  > Class=0 : 288277/298278 (96.6%)\n",
      "  > Class=1 : 10001/298278 (3.4%)\n",
      "WARNING:tensorflow:From /home/h/herbertm/.local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "    > F1: 99.694% (+/-0.097%)\n",
      "    > ROC-AUC: 99.991% (+/-0.016%)\n",
      "    > Average Precision: 99.820% (+/-0.067%)\n",
      "    > Balanced Accuracy: 99.710% (+/-0.099%)\n",
      "    > Precision: 99.970% (+/-0.046%)\n",
      "    > Recall: 99.420% (+/-0.199%)\n",
      "    > Fit Time: 272.545 s (+/-2.139 s)\n",
      "Undersampled Dataset\n",
      "  > Class=0 : 10001/20002 (50.0%)\n",
      "  > Class=1 : 10001/20002 (50.0%)\n",
      "    > F1: 99.256% (+/-0.228%)\n",
      "    > ROC-AUC: 99.991% (+/-0.016%)\n",
      "    > Average Precision: 99.990% (+/-0.016%)\n",
      "    > Balanced Accuracy: 99.250% (+/-0.231%)\n",
      "    > Precision: 98.584% (+/-0.578%)\n",
      "    > Recall: 99.940% (+/-0.150%)\n",
      "    > Fit Time: 26.430 s (+/-0.525 s)\n",
      "Oversampled Dataset\n",
      "  > Class=0 : 288277/576554 (50.0%)\n",
      "  > Class=1 : 288277/576554 (50.0%)\n",
      "    > F1: 99.716% (+/-0.028%)\n",
      "    > ROC-AUC: 99.991% (+/-0.004%)\n",
      "    > Average Precision: 99.990% (+/-0.003%)\n",
      "    > Balanced Accuracy: 99.717% (+/-0.028%)\n",
      "    > Precision: 99.999% (+/-0.001%)\n",
      "    > Recall: 99.434% (+/-0.056%)\n",
      "    > Fit Time: 524.582 s (+/-8.271 s)\n",
      "== Weighted DNN Model: == \n",
      "\n",
      "Metric,Original,Undersampled,Oversampled,Fit Time\n",
      "F1,0.99694,0.99256,0.99716\n",
      "ROC-AUC,0.99991,0.99991,0.99991\n",
      "Average Precision,0.99820,0.99990,0.99990\n",
      "Balanced Accuracy,0.99710,0.99250,0.99717\n",
      "Precision,0.99970,0.98584,0.99999\n",
      "Recall,0.99420,0.99940,0.99434\n",
      "Fit Time,272.54514,26.42985,524.58239\n"
     ]
    }
   ],
   "source": [
    "#%%writefile -a anomaly-detector.py\n",
    "\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0 (normal): {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1 (attack): {:.2f}'.format(weight_for_1))\n",
    "\n",
    "#estimators = []\n",
    "#model = KerasClassifier(build_fn=make_model, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0, class_weight=class_weight)\n",
    "#model._estimator_type = \"classifier\"\n",
    "#estimators.append(('standardize', StandardScaler()))\n",
    "#estimators.append(('weighted-model', model))\n",
    "#pipeline = Pipeline(estimators)\n",
    "\n",
    "weighted_orig = cross_validate_model(make_pipeline(StandardScaler(), KerasClassifier(build_fn=make_model, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0, class_weight=class_weight)), X, y, kfold, \"Original Dataset\")\n",
    "weighted_under = cross_validate_model(make_pipeline(StandardScaler(), KerasClassifier(build_fn=make_model, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0, class_weight=class_weight)), X_under, y_under, kfold, \"Undersampled Dataset\")\n",
    "weighted_over = cross_validate_model(make_pipeline(StandardScaler(), KerasClassifier(build_fn=make_model, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0, class_weight=class_weight)), X_over, y_over, kfold, \"Oversampled Dataset\")\n",
    "\n",
    "print_table(\"Weighted DNN Model\", weighted_orig, weighted_under, weighted_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Weighted DNN Model: == \n",
      "\n",
      "Metric,Original Mean,Original Std,Undersampled Mean,Undersampled Std,Oversampled Mean,Oversampled Std\n",
      "F1,0.99694,0.00097,0.99256,0.00228,0.99716,0.00028\n",
      "ROC-AUC,0.99991,0.00016,0.99991,0.00016,0.99991,0.00004\n",
      "Average Precision,0.99820,0.00067,0.99990,0.00016,0.99990,0.00003\n",
      "Balanced Accuracy,0.99710,0.00099,0.99250,0.00231,0.99717,0.00028\n",
      "Precision,0.99970,0.00046,0.98584,0.00578,0.99999,0.00001\n",
      "Recall,0.99420,0.00199,0.99940,0.00150,0.99434,0.00056\n",
      "Fit Time,272.54514,2.13897,26.42985,0.52505,524.58239,8.27065\n"
     ]
    }
   ],
   "source": [
    "print_table(\"Weighted DNN Model\", weighted_orig, weighted_under, weighted_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "imbalanced_data.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
